{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('C:/Users/Bruno/anaconda3/Lib/site-packages/Spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()\n",
    "\n",
    "# Monitor the logs directory for new log data, and read in the raw lines as accessLines\n",
    "accessLines = spark.readStream.text(\"C:/Users/Bruno/Desktop/Python/Projectos Big Data/Spark/logs\")\n",
    "\n",
    "# Parse out the common log format to a DataFrame\n",
    "contentSizeExp = r'\\s(\\d+)$'\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "\n",
    "logsDF = accessLines.select(func.regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         func.regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
    "                         func.regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         func.regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         func.regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         func.regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
    "                         func.regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))\n",
    "\n",
    "logsDF2 = logsDF.withColumn(\"eventTime\", func.current_timestamp())\n",
    "\n",
    "# Keep a running count of endpoints\n",
    "endpointCounts = logsDF2.groupBy(func.window(func.col(\"eventTime\"), \\\n",
    "      \"30 seconds\", \"10 seconds\"), func.col(\"endpoint\")).count()\n",
    "\n",
    "sortedEndpointCounts = endpointCounts.orderBy(func.col(\"count\").desc())\n",
    "\n",
    "# Display the stream to the console\n",
    "query = sortedEndpointCounts.writeStream.outputMode(\"complete\").format(\"console\") \\\n",
    "      .queryName(\"counts\").start()\n",
    "\n",
    "# Wait until we terminate the scripts\n",
    "query.awaitTermination()\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
